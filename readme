ğŸ§© Project Title & Description

Title: Online Retail Sales Analysis using SQL in Python (SQLite)

    Objective:
            To analyze online retail transaction data using SQL queries inside Python.
            The goal was to calculate total revenue, identify top products and customers, clean the dataset, and explore optimization strategies for SQL queries.

Problem Solved:
            Retail businesses often collect large amounts of transactional data but lack insights into product performance and customer spending. This project extracts actionable insights using SQL inside a data science workflow.

âš™ï¸ Methodology

      Tools Used:
          Python (Pandas, SQLite3)
          Jupyter/Colab Notebook
          Excel Dataset: Online_Retail.xlsx

Steps Followed:

          Loaded dataset and cleaned missing/duplicate data.
          Imported data into SQLite.
          Executed SQL queries to calculate:
          Total revenue
          Top products (by revenue & quantity)
          Top customers
          Monthly sales trend
          Optimized queries with indexing and filter-first grouping.

Sample SQL Queries:

        -- Total Revenue
        SELECT SUM(Quantity * UnitPrice) AS total_revenue FROM online_retail;
        
        -- Top 5 Products by Revenue
        SELECT StockCode, SUM(Quantity * UnitPrice) AS total_revenue
        FROM online_retail
        GROUP BY StockCode
        ORDER BY total_revenue DESC
        LIMIT 5;

ğŸ’¡ Challenges & Learnings

      Challenge: Date column (InvoiceDate) was stored as text in SQLite, causing sales trend queries to fail.
      Solution: Convert date column to datetime in pandas before inserting into SQLite.
      Learning: Efficient indexing and filtering significantly reduce query execution time in SQL databases.

ğŸ–¼ï¸ Visuals & Artifacts


ğŸ§¾ Project Title & Description

Title: Exploratory Data Analysis of Superstore Sales Dataset
Objective:
To perform detailed data analysis on a Superstore dataset, including missing value treatment, duplication checks, and insights on sales distribution.

Problem Solved:
EDA helps understand business performance and detect data quality issues before visualization or modeling.

âš™ï¸ Methodology

Tools Used: Python, Pandas, Colab

Dataset: Sample - Superstore.xls (Sheets: Orders, Returns, People)

Steps Followed:

Loaded all sheets using pandas.

Selected sheet with max rows (Orders) â†’ subset to 5000 rows.

Joined sheets using key attributes (Order ID, Region).

Performed data cleaning:

.describe() for numerical & categorical summary.

Checked for missing/duplicate values.

Filled missing values using mean/median/mode.

Filtered subset for Country = San Francisco and Sales > 1000.

Sample Code:

orders = pd.read_excel(file_path, sheet_name='Orders')
returns = pd.read_excel(file_path, sheet_name='Returns')
people = pd.read_excel(file_path, sheet_name='People')

# Join
final_df = orders.merge(returns, on='Order ID', how='left').merge(people, on='Region', how='left')


Notebook Link:
ğŸ”— Google Colab Notebook

ğŸ’¡ Challenges & Learnings

Challenge: Handling missing data while maintaining consistency across joins.

Solution: Applied column-wise imputation using mean, median, and mode, depending on data type.

Learning: Data quality strongly impacts insights; always check duplicates and null percentages before modeling.

ğŸ–¼ï¸ Visuals & Artifacts

ğŸ“Š Screenshot of .describe() summary

ğŸ“‰ Missing value percentage chart

ğŸ§® SQL + Python EDA code snapshot

ğŸ’» GitHub/Drive Link for notebook
(insert link after uploading your Colab notebook to GitHub or Google Drive)

ğŸ’¼ Freelance Pitch (Exercise 2 â€“ Portfolio Section)

â€œHi, I specialize in data cleaning and visualization for retail datasets.
In this Superstore project, I transformed raw Excel sheets into a unified analytics-ready dataset.
I can help you automate reports, detect missing values, and build interactive dashboards for decision-making.â€
